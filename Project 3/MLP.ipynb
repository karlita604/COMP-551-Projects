{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SuJRdH-uqkn_"
   },
   "source": [
    "In this module we will implement image classification through Multilayer Perceptron.\n",
    "Recall that a MLP - also known as a feed forward network, is multilayered. By the univeral approximation theorem: an MLP with single hidden layer can approximate any continuous function with arbitrary accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTOT6pP1H_5n"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch import matmul\n",
    "from torch.nn.init import xavier_uniform_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1LOJpPc3pfip"
   },
   "source": [
    "Load and normalizing the CIFAR10 training and test datasets using torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "lQ8oBavAoqQm",
    "outputId": "d63aae23-ed24-4fd8-88eb-708d7ef95524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84811e34b4648a8a215ef0f7ebe1f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# TAKEN FROM CONVNET IMPLEMENTATION\n",
    "\n",
    "# define transformation operation for corpus\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# download trainset, init dataloader\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# download testset, init dataloader\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# define possible classes\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zXHyMVfWCXjY"
   },
   "source": [
    " ## Our own implemented functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKVtnE1TCuyg"
   },
   "outputs": [],
   "source": [
    "#Xavier for weight initalization\n",
    "#m and h are the layer size dimensions\n",
    "def xavier(layer_size, next_layer_size, gain=1):\n",
    "  return torch.Tensor(layer_size, next_layer_size).uniform_(-1,1)*math.sqrt(6./(next_layer_size + layer_size))\n",
    "  # torch.Tensor(layer_size, next_layer_size).uniform_(-1/math.sqrt(layer_size), 1/math.sqrt(layer_size))\n",
    "  # return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NEjt5kOXCz12"
   },
   "outputs": [],
   "source": [
    "#Minibatch SGD with momentum (lecture 7, slide 7.2)\n",
    "def MinibatchSGD(X, y, lr=.01, eps=1e-2, bsize=4, beta=.99): \n",
    "  N,D = X.shape\n",
    "  w = np.zeros(D)\n",
    "  g = np.inf\n",
    "  dw=0\n",
    "  while np.linalg.norm(g) > eps:\n",
    "    minibatch = np.random.randint(N, size=(bsize)) \n",
    "    g = gradient(X[minibatch,:], y[minibatch], w) \n",
    "    dw = (1-beta)*g + beta*dw\n",
    "    w = w - lr*dw\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNy4azZGi92h"
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "  x_exp = 1 + torch.exp(-2 * x)\n",
    "  frac = (2/ x_exp)\n",
    "  return frac - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUAe5AOcP1In"
   },
   "outputs": [],
   "source": [
    "def tanh_deriv(x):\n",
    "  tanh_x = tanh(x)\n",
    "  tanh_sq = tanh_x * tanh_x\n",
    "  return 1 - tanh_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_67_J-bUBOQ"
   },
   "outputs": [],
   "source": [
    "# implement ReLU derivative\n",
    "def relu_deriv(x):\n",
    "  tens = torch.gt(x,0).float() #= 0 if torch.gt(0) else 1\n",
    "  # print(\"tens shape:\", tens.shape)\n",
    "  return tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "duCAwprNCy85"
   },
   "outputs": [],
   "source": [
    "#implement ReLU for middle layers, don't use sigmoid\n",
    "def relu(x):\n",
    "  relu_tensor = torch.clamp(x, min=0)\n",
    "  return relu_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wl2lBqjkCykW"
   },
   "outputs": [],
   "source": [
    "#Softmax (for final layer)\n",
    "def softmax(u):  # N x C\n",
    "  # for i in range(u.shape[0]):\n",
    "  #   u[i] = torch.add(u[i], -torch.max(u[i]))\n",
    "  # maximum = torch.max(u)\n",
    "  # u = torch.add(u, -maximum)\n",
    "  means = torch.mean(u,1,keepdim=True)\n",
    "  # u_exp = torch.exp(torch.clamp(u, min=torch.min(means), max=torch.max(means))) #[:, None])\n",
    "  # print(\"u:\", u)\n",
    "  u_exp = torch.exp(u-means) #, min=torch.min(means), max=torch.max(means)))\n",
    "  # print(\"u_exp:\", u_exp)\n",
    "  # print(u_exp.shape)\n",
    "  u_exp_sum = torch.sum(u_exp, dim=1, keepdim=True)\n",
    "  # print(denom.shape)\n",
    "  # frac = u_exp / denom\n",
    "  # for i in range(u_exp.shape[0]):\n",
    "  #   u_exp[i] = torch.div(u_exp[i], denom[i])\n",
    "  # frac = torch.div(u_exp, denom)\n",
    "  # print(u_exp)\n",
    "  frac = u_exp/u_exp_sum\n",
    "  # print(\"frac:\",frac)\n",
    "  return frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KtJwNbOb9Q0u"
   },
   "outputs": [],
   "source": [
    "def error(pred, label):\n",
    "  n_samples = label.shape[0]\n",
    "  logp = - torch.log(pred[np.arange(n_samples), torch.max(label,0)[1]])\n",
    "  loss = torch.sum(logp) / n_samples\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ga6AcCjy-pGJ"
   },
   "outputs": [],
   "source": [
    "def cross_entropy(pred, labels):\n",
    "  labels_onehot = labels_to_onehot(labels)\n",
    "  n_samples = pred.shape[0]\n",
    "  loss = 0\n",
    "  for i in range(n_samples):\n",
    "    for j in range(pred.shape[1]):\n",
    "      y = labels_onehot[i][j]\n",
    "      yhat = pred[i][j]\n",
    "      try:\n",
    "        if y == 1:\n",
    "          loss += -math.log(yhat)\n",
    "        else:\n",
    "          loss += -math.log(1 - yhat)\n",
    "      except ValueError:\n",
    "        print(\"ValueError:\", yhat)\n",
    "  loss = loss / (n_samples * pred.shape[1])\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PKlYFY7WbffF"
   },
   "outputs": [],
   "source": [
    "def delta_cross_entropy(inp, out):\n",
    "  m = out.shape[0]\n",
    "  grad = inp\n",
    "  grad[range(m),out] -= 1\n",
    "  grad = grad/m\n",
    "  return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySaY5nGzXR5C"
   },
   "outputs": [],
   "source": [
    "def labels_to_onehot(labels):\n",
    "  onehot = torch.zeros([labels.shape[0],10], dtype=torch.int32).cuda()\n",
    "  for i in range(labels.shape[0]):\n",
    "    onehot[i][labels[i]] = 1\n",
    "  # print(onehot)\n",
    "  return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bGFW6flYd2r"
   },
   "outputs": [],
   "source": [
    "def softmax_deriv(outputs, labels_onehot):\n",
    "  return outputs - labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J5VreksBLz2v"
   },
   "outputs": [],
   "source": [
    "def dropout(Wx, drop_probability):\n",
    "  keep_probability  = 1 - drop_probability\n",
    "  mask = torch.Tensor(Wx.shape).uniform_(0,1).cuda() < keep_probability\n",
    "  if keep_probability > 0:\n",
    "    scale = keep_probability\n",
    "  else:\n",
    "    scale = 0.0\n",
    "  out = scale * (mask * Wx)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dugfo9_oK4E"
   },
   "outputs": [],
   "source": [
    "def loss(labels_onehot, outputs):\n",
    "  loss = -torch.log(labels_onehot * outputs)\n",
    "  loss = torch.sum(loss) / labels_onehot.shape[0]\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NsMFBU1UHRBX"
   },
   "outputs": [],
   "source": [
    "def clip_grad(grad, threshold):\n",
    "  if threshold != None:\n",
    "    grad = torch.clamp(grad, min=-threshold, max=threshold)\n",
    "  return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAW-Xc74ChuF"
   },
   "source": [
    "### Add GPU as device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dUc6VP-5jGZg"
   },
   "outputs": [],
   "source": [
    "# define device as GPU (if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8LjUxNWjKc_"
   },
   "source": [
    "# Define a Multi-Layer Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5uI5QY-1Hl6G"
   },
   "outputs": [],
   "source": [
    "class own_MLP(nn.Module):\n",
    "\n",
    "  def __init__(self,\n",
    "               hidden_sizes,\n",
    "               lr,\n",
    "               grad_clip, \n",
    "               drop_wt=0,\n",
    "               schedule=False):\n",
    "    super(own_MLP, self).__init__()\n",
    "    self.hidden_sizes = hidden_sizes\n",
    "    self.lr = lr\n",
    "    self.grad_clip = grad_clip\n",
    "    self.drop_wt = drop_wt\n",
    "    self.schedule = schedule\n",
    "    self.lr_sched = 0.1\n",
    "\n",
    "    # create weight tensors, init with xavier\n",
    "    self.w1 = xavier(32*32*3, self.hidden_sizes[0]).cuda()\n",
    "    self.w2 = xavier(self.hidden_sizes[0], self.hidden_sizes[1]).cuda()\n",
    "    self.w3 = xavier(self.hidden_sizes[1], self.hidden_sizes[2]).cuda()\n",
    "    self.w4 = xavier(self.hidden_sizes[2], 10).cuda()\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # do forward pass\n",
    "    x = x.view(-1, 32*32*3)\n",
    "    self.z1 = matmul(x, self.w1)\n",
    "    self.a1 = relu(self.z1)\n",
    "    self.z2 = matmul(self.a1, self.w2)\n",
    "    self.a2 = relu(self.z2)\n",
    "    self.z3 = matmul(self.a2, self.w3)\n",
    "    self.a3 = relu(self.z3)\n",
    "    self.z4 = matmul(self.a3, self.w4)\n",
    "    self.a4 = softmax(self.z4)\n",
    "    return self.a4\n",
    "\n",
    "  def backprop(self, X, labels, outputs):\n",
    "    if self.schedule:\n",
    "      self.learning_rate = self.learning_rate * 0.8\n",
    "    X = X.view(-1, 3*32*32)\n",
    "\n",
    "    # calculate derivatives\n",
    "    self.a4_delta = clip_grad(outputs - labels_to_onehot(labels), self.grad_clip)\n",
    "    self.a3_delta = clip_grad(relu_deriv(self.a3) * matmul(self.a4_delta, self.w4.T), self.grad_clip)\n",
    "    self.a2_delta = clip_grad(relu_deriv(self.a2) * matmul(self.a3_delta, self.w3.T), self.grad_clip)\n",
    "    self.a1_delta = clip_grad(relu_deriv(self.a1) * matmul(self.a2_delta, self.w2.T), self.grad_clip)\n",
    "\n",
    "    # update the weights\n",
    "    self.w4 -= self.lr * matmul(self.a3.T, self.a4_delta)\n",
    "    self.w3 -= self.lr * matmul(self.a2.T, self.a3_delta)\n",
    "    self.w2 -= self.lr * matmul(self.a1.T, self.a2_delta)\n",
    "    self.w1 -= self.lr * matmul(X.T, self.a1_delta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7fmvL3HtkYq"
   },
   "outputs": [],
   "source": [
    "class pytorch_MLP(nn.Module):\n",
    "\n",
    "  def __init__(self,\n",
    "               hidden_size):\n",
    "    super(pytorch_MLP, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    self.h1 = torch.nn.Linear(32*32*3, self.hidden_size)\n",
    "    self.h2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "    self.h3 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "    self.h4 = torch.nn.Linear(self.hidden_size, 10)\n",
    "\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    self.softmax = torch.nn.Softmax()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(-1, 32*32*3)\n",
    "    x.to(device)\n",
    "    x = self.relu(self.h1(x))\n",
    "    x = self.relu(self.h2(x))\n",
    "    x = self.relu(self.h3(x))\n",
    "    x = self.softmax(self.h4(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUd2dQehvXTI"
   },
   "outputs": [],
   "source": [
    "## CURRENTLY UNUSABLE!!\n",
    "#   (implement if have time, for more complex experiments)\n",
    "\n",
    "class complex_MLP(nn.Module):\n",
    "  def __init__(self,\n",
    "               layer_size, \n",
    "               activation_funct):\n",
    "    super(simple_MLP, self).__init__()\n",
    "    self.layer_size = layer_size\n",
    "    self.n_layers = len(layer_size)\n",
    "    self.activation_funct = activation_funct\n",
    "\n",
    "    for i in range(n_layers):\n",
    "      if i == 0:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7cGZqz6F9Vp"
   },
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6cd7d62lxj1G"
   },
   "outputs": [],
   "source": [
    "def run_experiment(hidden_list, NUM_EPOCHS, lr=0.0001, grad_clip=None, drop_wt=0):\n",
    "  mlp = own_MLP(hidden_sizes=hidden_list, lr=lr, grad_clip=grad_clip, drop_wt=drop_wt)\n",
    "  mlp.to(device)\n",
    "\n",
    "  acc_dict = {}\n",
    "  \n",
    "  # TRAIN !\n",
    "  for epoch in range(NUM_EPOCHS):\n",
    "    acc_list = []\n",
    "    # reset loss to 0 at start of each epoch\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # iterate through the data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "      # load inputs and labels from dataloader, put on GPU\n",
    "      inputs, labels = data[0].to(device), data[1].to(device)\n",
    "      \n",
    "      # forward pass\n",
    "      outputs = mlp(inputs)\n",
    "\n",
    "      # calc loss based on the outputs and ground truth\n",
    "      loss = cross_entropy(outputs, labels)\n",
    "     \n",
    "      # calc the gradients through the network, update weights\n",
    "      mlp.backprop(inputs, labels, outputs)\n",
    "      \n",
    "      running_loss += loss #.item()\n",
    "      # every 2000 steps, print average loss and zero out running loss\n",
    "      if i % 2000 == 1999:\n",
    "        print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch +1, i+1, running_loss / 2000))\n",
    "        running_loss = 0.0\n",
    "    \n",
    "    # UPDATE LEARNING RATE \n",
    "    mlp.lr = mlp.lr_sched * mlp.lr\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # don't update gradients (leftover from pytorch implementation, does nothing now)\n",
    "    with torch.no_grad():\n",
    "      # iterate through test dataset\n",
    "      for data in trainloader:\n",
    "        # load test data, put on GPU\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # get outputs\n",
    "        outputs = mlp(images)\n",
    "        # predict category based on outputs\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # increase total number of labels by number of instances seen in this batch\n",
    "        total += labels.size(0)\n",
    "        # increase number of correct predicitons by number of correct predictions in this batch\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    # calc and print overall model accuracy\n",
    "    acc = 100 * correct / total\n",
    "    acc_list.append(acc)\n",
    "    print('Accuracy of the network on train images: %f %%' % (acc))\n",
    "  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # don't update gradients\n",
    "    with torch.no_grad():\n",
    "      # iterate through test dataset\n",
    "      for data in testloader:\n",
    "        # load test data, put on GPU\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # get outputs\n",
    "        outputs = mlp(images)\n",
    "        # predict category based on outputs\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # increase total number of labels by number of instances seen in this batch\n",
    "        total += labels.size(0)\n",
    "        # increase number of correct predicitons by number of correct predictions in this batch\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    # calc and print overall model accuracy\n",
    "    acc = 100 * correct / total\n",
    "    acc_list.append(acc)\n",
    "    print('Accuracy of the network on test images: %f %%' % (acc))\n",
    "\n",
    "    acc_dict[epoch] = acc_list\n",
    "  print('Finished Training!')\n",
    "\n",
    "  # PREDICT AND CALCULATE ACCURACY\n",
    "  # count number of correct and number of total\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  # don't update gradients\n",
    "  with torch.no_grad():\n",
    "    # iterate through test dataset\n",
    "    for data in testloader:\n",
    "      # load test data, put on GPU\n",
    "      images, labels = data[0].to(device), data[1].to(device)\n",
    "      # get outputs\n",
    "      outputs = mlp(images)\n",
    "      # predict category based on outputs\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      # increase total number of labels by number of instances seen in this batch\n",
    "      total += labels.size(0)\n",
    "      # increase number of correct predicitons by number of correct predictions in this batch\n",
    "      correct += (predicted == labels).sum().item()\n",
    "  # calc and print overall model accuracy\n",
    "  acc = 100 * correct / total\n",
    "  print('Accuracy of the network on the 10000 test images: %f %%' % (acc))\n",
    "  return acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "colab_type": "code",
    "id": "f44wuz1Ci00y",
    "outputId": "c971ff3c-e017-4626-dc19-7afc7bccc0e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.273\n",
      "[1,  4000] loss: 0.251\n",
      "[1,  6000] loss: 0.243\n",
      "[1,  8000] loss: 0.239\n",
      "[1, 10000] loss: 0.234\n",
      "[1, 12000] loss: 0.231\n",
      "Accuracy of the network on train images: 50.626000 %\n",
      "Accuracy of the network on test images: 47.870000 %\n",
      "[2,  2000] loss: 0.213\n",
      "[2,  4000] loss: 0.208\n",
      "[2,  6000] loss: 0.206\n",
      "[2,  8000] loss: 0.202\n",
      "[2, 10000] loss: 0.203\n",
      "[2, 12000] loss: 0.201\n",
      "Accuracy of the network on train images: 55.818000 %\n",
      "Accuracy of the network on test images: 51.810000 %\n",
      "[3,  2000] loss: 0.197\n",
      "[3,  4000] loss: 0.198\n",
      "[3,  6000] loss: 0.198\n",
      "[3,  8000] loss: 0.197\n",
      "[3, 10000] loss: 0.197\n",
      "[3, 12000] loss: 0.195\n",
      "Accuracy of the network on train images: 56.332000 %\n",
      "Accuracy of the network on test images: 52.000000 %\n",
      "[4,  2000] loss: 0.197\n",
      "[4,  4000] loss: 0.196\n",
      "[4,  6000] loss: 0.195\n",
      "[4,  8000] loss: 0.196\n",
      "[4, 10000] loss: 0.197\n",
      "[4, 12000] loss: 0.196\n",
      "Accuracy of the network on train images: 56.332000 %\n",
      "Accuracy of the network on test images: 52.030000 %\n",
      "[5,  2000] loss: 0.194\n",
      "[5,  4000] loss: 0.199\n",
      "[5,  6000] loss: 0.196\n",
      "[5,  8000] loss: 0.195\n",
      "[5, 10000] loss: 0.195\n",
      "[5, 12000] loss: 0.197\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[6,  2000] loss: 0.195\n",
      "[6,  4000] loss: 0.198\n",
      "[6,  6000] loss: 0.194\n",
      "[6,  8000] loss: 0.198\n",
      "[6, 10000] loss: 0.196\n",
      "[6, 12000] loss: 0.196\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[7,  2000] loss: 0.196\n",
      "[7,  4000] loss: 0.197\n",
      "[7,  6000] loss: 0.198\n",
      "[7,  8000] loss: 0.196\n",
      "[7, 10000] loss: 0.194\n",
      "[7, 12000] loss: 0.195\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[8,  2000] loss: 0.196\n",
      "[8,  4000] loss: 0.196\n",
      "[8,  6000] loss: 0.198\n",
      "[8,  8000] loss: 0.197\n",
      "[8, 10000] loss: 0.195\n",
      "[8, 12000] loss: 0.193\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[9,  2000] loss: 0.197\n",
      "[9,  4000] loss: 0.195\n",
      "[9,  6000] loss: 0.196\n",
      "[9,  8000] loss: 0.196\n",
      "[9, 10000] loss: 0.196\n",
      "[9, 12000] loss: 0.197\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[10,  2000] loss: 0.195\n",
      "[10,  4000] loss: 0.196\n",
      "[10,  6000] loss: 0.196\n",
      "[10,  8000] loss: 0.197\n",
      "[10, 10000] loss: 0.195\n",
      "[10, 12000] loss: 0.196\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[11,  2000] loss: 0.195\n",
      "[11,  4000] loss: 0.197\n",
      "[11,  6000] loss: 0.195\n",
      "[11,  8000] loss: 0.195\n",
      "[11, 10000] loss: 0.195\n",
      "[11, 12000] loss: 0.199\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[12,  2000] loss: 0.196\n",
      "[12,  4000] loss: 0.194\n",
      "[12,  6000] loss: 0.195\n",
      "[12,  8000] loss: 0.197\n",
      "[12, 10000] loss: 0.196\n",
      "[12, 12000] loss: 0.198\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[13,  2000] loss: 0.196\n",
      "[13,  4000] loss: 0.195\n",
      "[13,  6000] loss: 0.197\n",
      "[13,  8000] loss: 0.196\n",
      "[13, 10000] loss: 0.196\n",
      "[13, 12000] loss: 0.195\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[14,  2000] loss: 0.196\n",
      "[14,  4000] loss: 0.193\n",
      "[14,  6000] loss: 0.194\n",
      "[14,  8000] loss: 0.198\n",
      "[14, 10000] loss: 0.195\n",
      "[14, 12000] loss: 0.200\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[15,  2000] loss: 0.197\n",
      "[15,  4000] loss: 0.194\n",
      "[15,  6000] loss: 0.198\n",
      "[15,  8000] loss: 0.195\n",
      "[15, 10000] loss: 0.193\n",
      "[15, 12000] loss: 0.197\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[16,  2000] loss: 0.196\n",
      "[16,  4000] loss: 0.197\n",
      "[16,  6000] loss: 0.196\n",
      "[16,  8000] loss: 0.197\n",
      "[16, 10000] loss: 0.194\n",
      "[16, 12000] loss: 0.196\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[17,  2000] loss: 0.196\n",
      "[17,  4000] loss: 0.194\n",
      "[17,  6000] loss: 0.195\n",
      "[17,  8000] loss: 0.197\n",
      "[17, 10000] loss: 0.197\n",
      "[17, 12000] loss: 0.196\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[18,  2000] loss: 0.196\n",
      "[18,  4000] loss: 0.193\n",
      "[18,  6000] loss: 0.199\n",
      "[18,  8000] loss: 0.196\n",
      "[18, 10000] loss: 0.198\n",
      "[18, 12000] loss: 0.194\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[19,  2000] loss: 0.198\n",
      "[19,  4000] loss: 0.198\n",
      "[19,  6000] loss: 0.193\n",
      "[19,  8000] loss: 0.195\n",
      "[19, 10000] loss: 0.194\n",
      "[19, 12000] loss: 0.198\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[20,  2000] loss: 0.195\n",
      "[20,  4000] loss: 0.199\n",
      "[20,  6000] loss: 0.195\n",
      "[20,  8000] loss: 0.196\n",
      "[20, 10000] loss: 0.195\n",
      "[20, 12000] loss: 0.196\n",
      "Accuracy of the network on train images: 56.328000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "Finished Training!\n",
      "Accuracy of the network on the 10000 test images: 52.040000 %\n"
     ]
    }
   ],
   "source": [
    "acc_dict = run_experiment([1024, 512, 128], 20, lr=0.001, grad_clip=1, drop_wt=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OLkitquji1O-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [50.626, 47.87], 1: [55.818, 51.81], 2: [56.332, 52.0], 3: [56.332, 52.03], 4: [56.328, 52.04], 5: [56.328, 52.04], 6: [56.328, 52.04], 7: [56.328, 52.04], 8: [56.328, 52.04], 9: [56.328, 52.04], 10: [56.328, 52.04], 11: [56.328, 52.04], 12: [56.328, 52.04], 13: [56.328, 52.04], 14: [56.328, 52.04], 15: [56.328, 52.04], 16: [56.328, 52.04], 17: [56.328, 52.04], 18: [56.328, 52.04], 19: [56.328, 52.04]}\n"
     ]
    }
   ],
   "source": [
    "print(acc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SCvqNTeEj7e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1024, 512, 128], NUM_EPOCHS=40, lr=0.001, grad_clip=1, drop_wt=0)\n",
      "{0: [50.626, 47.87], 1: [55.818, 51.81], 2: [56.332, 52.0], 3: [56.332, 52.03], 4: [56.328, 52.04], 5: [56.328, 52.04], 6: [56.328, 52.04], 7: [56.328, 52.04], 8: [56.328, 52.04], 9: [56.328, 52.04], 10: [56.328, 52.04], 11: [56.328, 52.04], 12: [56.328, 52.04], 13: [56.328, 52.04], 14: [56.328, 52.04], 15: [56.328, 52.04], 16: [56.328, 52.04], 17: [56.328, 52.04], 18: [56.328, 52.04], 19: [56.328, 52.04]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# code for graphing epochs vs acc for train vs. test\n",
    "import seaborn as sns\n",
    "\n",
    "datalist = []\n",
    "\n",
    "for key in acc_dict.keys():\n",
    "  currlist_train = [key, acc_dict[key][0], 'Train']\n",
    "  datalist.append(currlist_train)\n",
    "  currlist_test = [key, acc_dict[key][1], 'Test']\n",
    "  datalist.append(currlist_test)\n",
    "\n",
    "acc_df = pd.DataFrame(datalist, columns=['Epoch', 'Accuracy', 'Dataset'])\n",
    "model_spec = \"[1024, 512, 128], NUM_EPOCHS=40, lr=0.001, grad_clip=1, drop_wt=0)\"\n",
    "print(model_spec)\n",
    "print(acc_dict)\n",
    "\n",
    "g = sns.lineplot(x=\"Epoch\", y=\"Accuracy\", hue=\"Dataset\", palette=\"YlGnBu_d\", data=acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EKWbNnmWx4pa",
    "outputId": "8a89a7f3-eab5-420a-a75b-0e8bd3cae90b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.262\n",
      "[1,  4000] loss: 0.241\n",
      "[1,  6000] loss: 0.235\n",
      "Accuracy of the network on train images: 50.628000 %\n",
      "Accuracy of the network on test images: 47.860000 %\n",
      "[2,  2000] loss: 0.217\n",
      "[2,  4000] loss: 0.214\n",
      "[2,  6000] loss: 0.213\n",
      "Accuracy of the network on train images: 56.872000 %\n",
      "Accuracy of the network on test images: 50.530000 %\n",
      "[3,  2000] loss: 0.198\n",
      "[3,  4000] loss: 0.198\n",
      "[3,  6000] loss: 0.197\n",
      "Accuracy of the network on train images: 61.744000 %\n",
      "Accuracy of the network on test images: 52.750000 %\n",
      "[4,  2000] loss: 0.180\n",
      "[4,  4000] loss: 0.183\n",
      "[4,  6000] loss: 0.184\n",
      "Accuracy of the network on train images: 62.456000 %\n",
      "Accuracy of the network on test images: 50.990000 %\n",
      "[5,  2000] loss: 0.163\n",
      "[5,  4000] loss: 0.169\n",
      "[5,  6000] loss: 0.172\n",
      "Accuracy of the network on train images: 69.150000 %\n",
      "Accuracy of the network on test images: 53.910000 %\n",
      "[6,  2000] loss: 0.151\n",
      "[6,  4000] loss: 0.156\n",
      "[6,  6000] loss: 0.158\n",
      "Accuracy of the network on train images: 71.868000 %\n",
      "Accuracy of the network on test images: 53.510000 %\n",
      "[7,  2000] loss: 0.134\n",
      "[7,  4000] loss: 0.142\n",
      "[7,  6000] loss: 0.147\n",
      "Accuracy of the network on train images: 75.200000 %\n",
      "Accuracy of the network on test images: 53.000000 %\n",
      "[8,  2000] loss: 0.124\n",
      "[8,  4000] loss: 0.129\n",
      "[8,  6000] loss: 0.134\n",
      "Accuracy of the network on train images: 78.528000 %\n",
      "Accuracy of the network on test images: 53.770000 %\n",
      "[9,  2000] loss: 0.109\n",
      "[9,  4000] loss: 0.116\n",
      "[9,  6000] loss: 0.125\n",
      "Accuracy of the network on train images: 80.520000 %\n",
      "Accuracy of the network on test images: 53.330000 %\n",
      "[10,  2000] loss: 0.096\n",
      "[10,  4000] loss: 0.106\n",
      "[10,  6000] loss: 0.113\n",
      "Accuracy of the network on train images: 83.170000 %\n",
      "Accuracy of the network on test images: 52.260000 %\n",
      "[11,  2000] loss: 0.085\n",
      "[11,  4000] loss: 0.093\n",
      "[11,  6000] loss: 0.101\n",
      "Accuracy of the network on train images: 85.150000 %\n",
      "Accuracy of the network on test images: 53.670000 %\n",
      "[12,  2000] loss: 0.077\n",
      "[12,  4000] loss: 0.085\n",
      "[12,  6000] loss: 0.091\n",
      "Accuracy of the network on train images: 85.798000 %\n",
      "Accuracy of the network on test images: 53.330000 %\n",
      "[13,  2000] loss: 0.068\n",
      "[13,  4000] loss: 0.076\n",
      "[13,  6000] loss: 0.082\n",
      "Accuracy of the network on train images: 88.908000 %\n",
      "Accuracy of the network on test images: 53.520000 %\n",
      "[14,  2000] loss: 0.059\n",
      "[14,  4000] loss: 0.067\n",
      "[14,  6000] loss: 0.075\n",
      "Accuracy of the network on train images: 86.924000 %\n",
      "Accuracy of the network on test images: 52.470000 %\n",
      "[15,  2000] loss: 0.053\n",
      "[15,  4000] loss: 0.062\n",
      "[15,  6000] loss: 0.069\n",
      "Accuracy of the network on train images: 90.876000 %\n",
      "Accuracy of the network on test images: 53.480000 %\n",
      "[16,  2000] loss: 0.047\n",
      "[16,  4000] loss: 0.054\n",
      "[16,  6000] loss: 0.061\n",
      "Accuracy of the network on train images: 90.986000 %\n",
      "Accuracy of the network on test images: 53.420000 %\n",
      "[17,  2000] loss: 0.046\n",
      "[17,  4000] loss: 0.052\n",
      "[17,  6000] loss: 0.058\n",
      "Accuracy of the network on train images: 91.040000 %\n",
      "Accuracy of the network on test images: 52.570000 %\n",
      "[18,  2000] loss: 0.041\n",
      "[18,  4000] loss: 0.044\n",
      "[18,  6000] loss: 0.048\n",
      "Accuracy of the network on train images: 92.306000 %\n",
      "Accuracy of the network on test images: 52.870000 %\n",
      "[19,  2000] loss: 0.035\n",
      "[19,  4000] loss: 0.042\n",
      "[19,  6000] loss: 0.047\n",
      "Accuracy of the network on train images: 93.036000 %\n",
      "Accuracy of the network on test images: 53.340000 %\n",
      "[20,  2000] loss: 0.032\n",
      "[20,  4000] loss: 0.040\n",
      "[20,  6000] loss: 0.045\n",
      "Accuracy of the network on train images: 91.736000 %\n",
      "Accuracy of the network on test images: 52.040000 %\n",
      "[21,  2000] loss: 0.032\n",
      "[21,  4000] loss: 0.038\n",
      "[21,  6000] loss: 0.037\n",
      "Accuracy of the network on train images: 93.634000 %\n",
      "Accuracy of the network on test images: 52.990000 %\n",
      "[22,  2000] loss: 0.027\n",
      "[22,  4000] loss: 0.037\n",
      "[22,  6000] loss: 0.037\n",
      "Accuracy of the network on train images: 95.172000 %\n",
      "Accuracy of the network on test images: 53.770000 %\n",
      "[23,  2000] loss: 0.026\n",
      "[23,  4000] loss: 0.034\n",
      "[23,  6000] loss: 0.039\n",
      "Accuracy of the network on train images: 94.314000 %\n",
      "Accuracy of the network on test images: 52.800000 %\n",
      "[24,  2000] loss: 0.026\n",
      "[24,  4000] loss: 0.031\n",
      "[24,  6000] loss: 0.036\n",
      "Accuracy of the network on train images: 95.124000 %\n",
      "Accuracy of the network on test images: 53.310000 %\n",
      "[25,  2000] loss: 0.023\n",
      "[25,  4000] loss: 0.027\n",
      "[25,  6000] loss: 0.033\n",
      "Accuracy of the network on train images: 94.810000 %\n",
      "Accuracy of the network on test images: 53.080000 %\n",
      "[26,  2000] loss: nan\n",
      "[26,  4000] loss: nan\n",
      "[26,  6000] loss: nan\n",
      "Accuracy of the network on train images: 10.000000 %\n",
      "Accuracy of the network on test images: 10.000000 %\n",
      "[27,  2000] loss: nan\n",
      "[27,  4000] loss: nan\n",
      "[27,  6000] loss: nan\n",
      "Accuracy of the network on train images: 10.000000 %\n",
      "Accuracy of the network on test images: 10.000000 %\n",
      "[28,  2000] loss: nan\n",
      "[28,  4000] loss: nan\n",
      "[28,  6000] loss: nan\n",
      "Accuracy of the network on train images: 10.000000 %\n",
      "Accuracy of the network on test images: 10.000000 %\n",
      "[29,  2000] loss: nan\n",
      "[29,  4000] loss: nan\n",
      "[29,  6000] loss: nan\n",
      "Accuracy of the network on train images: 10.000000 %\n",
      "Accuracy of the network on test images: 10.000000 %\n",
      "[30,  2000] loss: nan\n",
      "[30,  4000] loss: nan\n",
      "[30,  6000] loss: nan\n",
      "Accuracy of the network on train images: 10.000000 %\n",
      "Accuracy of the network on test images: 10.000000 %\n",
      "[31,  2000] loss: nan\n",
      "[31,  4000] loss: nan\n",
      "[31,  6000] loss: nan\n",
      "Accuracy of the network on train images: 10.000000 %\n",
      "Accuracy of the network on test images: 10.000000 %\n",
      "[32,  2000] loss: nan\n",
      "[32,  4000] loss: nan\n",
      "[32,  6000] loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-b82824c92057>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_wt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-ce0fd4be0aae>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(hidden_list, NUM_EPOCHS, lr, grad_clip, drop_wt)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# load test data, put on GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;31m# get outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc_dict = run_experiment([1024, 512, 128], 120, lr=0.001, grad_clip=500, drop_wt=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cU-RtRivA7F_",
    "outputId": "a993583c-152d-4edc-fc09-124a83bcc06b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.302\n",
      "[1,  4000] loss: 0.279\n",
      "[1,  6000] loss: 0.267\n",
      "[1,  8000] loss: 0.262\n",
      "[1, 10000] loss: 0.257\n",
      "[1, 12000] loss: 0.257\n",
      "Accuracy of the network on train images: 42 %\n",
      "Accuracy of the network on test images: 41 %\n",
      "[2,  2000] loss: 0.244\n",
      "[2,  4000] loss: 0.247\n",
      "[2,  6000] loss: 0.241\n",
      "[2,  8000] loss: 0.243\n",
      "[2, 10000] loss: 0.241\n",
      "[2, 12000] loss: 0.239\n",
      "Accuracy of the network on train images: 46 %\n",
      "Accuracy of the network on test images: 44 %\n",
      "[3,  2000] loss: 0.234\n",
      "[3,  4000] loss: 0.233\n",
      "[3,  6000] loss: 0.231\n",
      "[3,  8000] loss: 0.230\n",
      "[3, 10000] loss: 0.230\n",
      "[3, 12000] loss: 0.230\n",
      "Accuracy of the network on train images: 49 %\n",
      "Accuracy of the network on test images: 47 %\n",
      "[4,  2000] loss: 0.223\n",
      "[4,  4000] loss: 0.225\n",
      "[4,  6000] loss: 0.225\n",
      "[4,  8000] loss: 0.223\n",
      "[4, 10000] loss: 0.224\n",
      "[4, 12000] loss: 0.221\n",
      "Accuracy of the network on train images: 51 %\n",
      "Accuracy of the network on test images: 47 %\n",
      "[5,  2000] loss: 0.217\n",
      "[5,  4000] loss: 0.216\n",
      "[5,  6000] loss: 0.218\n",
      "[5,  8000] loss: 0.217\n",
      "[5, 10000] loss: 0.215\n",
      "[5, 12000] loss: 0.219\n",
      "Accuracy of the network on train images: 53 %\n",
      "Accuracy of the network on test images: 48 %\n",
      "[6,  2000] loss: 0.211\n",
      "[6,  4000] loss: 0.210\n",
      "[6,  6000] loss: 0.212\n",
      "[6,  8000] loss: 0.212\n",
      "[6, 10000] loss: 0.211\n",
      "[6, 12000] loss: 0.209\n",
      "Accuracy of the network on train images: 54 %\n",
      "Accuracy of the network on test images: 49 %\n",
      "[7,  2000] loss: 0.206\n",
      "[7,  4000] loss: 0.206\n",
      "[7,  6000] loss: 0.207\n",
      "[7,  8000] loss: 0.204\n",
      "[7, 10000] loss: 0.205\n",
      "[7, 12000] loss: 0.205\n",
      "Accuracy of the network on train images: 56 %\n",
      "Accuracy of the network on test images: 50 %\n",
      "[8,  2000] loss: 0.201\n",
      "[8,  4000] loss: 0.201\n",
      "[8,  6000] loss: 0.196\n",
      "[8,  8000] loss: 0.200\n",
      "[8, 10000] loss: 0.203\n",
      "[8, 12000] loss: 0.201\n",
      "Accuracy of the network on train images: 57 %\n",
      "Accuracy of the network on test images: 51 %\n",
      "[9,  2000] loss: 0.196\n",
      "[9,  4000] loss: 0.195\n",
      "[9,  6000] loss: 0.195\n",
      "[9,  8000] loss: 0.195\n",
      "[9, 10000] loss: 0.199\n",
      "[9, 12000] loss: 0.193\n",
      "Accuracy of the network on train images: 58 %\n",
      "Accuracy of the network on test images: 51 %\n",
      "[10,  2000] loss: 0.191\n",
      "[10,  4000] loss: 0.192\n",
      "[10,  6000] loss: 0.190\n",
      "[10,  8000] loss: 0.189\n",
      "[10, 10000] loss: 0.191\n",
      "[10, 12000] loss: 0.192\n",
      "Accuracy of the network on train images: 60 %\n",
      "Accuracy of the network on test images: 51 %\n",
      "Finished Training!\n",
      "Accuracy of the network on the 10000 test images: 51 %\n"
     ]
    }
   ],
   "source": [
    "run_experiment([1024, 128], 10, lr=0.0001, grad_clip=1, drop_wt=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "W9TGoQssx7nq",
    "outputId": "b9d4c893-13df-492e-fd68-95684fa22b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.270\n",
      "[1,  4000] loss: 0.252\n",
      "[1,  6000] loss: 0.242\n",
      "[1,  8000] loss: 0.241\n",
      "[1, 10000] loss: 0.234\n",
      "[1, 12000] loss: 0.229\n",
      "Accuracy of the network on train images: 49 %\n",
      "Accuracy of the network on test images: 47 %\n",
      "[2,  2000] loss: 0.214\n",
      "[2,  4000] loss: 0.215\n",
      "[2,  6000] loss: 0.214\n",
      "[2,  8000] loss: 0.214\n",
      "[2, 10000] loss: 0.216\n",
      "[2, 12000] loss: 0.213\n",
      "Accuracy of the network on train images: 56 %\n",
      "Accuracy of the network on test images: 50 %\n",
      "[3,  2000] loss: 0.200\n",
      "[3,  4000] loss: 0.199\n",
      "[3,  6000] loss: 0.196\n",
      "[3,  8000] loss: 0.198\n",
      "[3, 10000] loss: 0.200\n",
      "[3, 12000] loss: 0.196\n",
      "Accuracy of the network on train images: 60 %\n",
      "Accuracy of the network on test images: 51 %\n",
      "[4,  2000] loss: 0.185\n",
      "[4,  4000] loss: 0.184\n",
      "[4,  6000] loss: 0.184\n",
      "[4,  8000] loss: 0.185\n",
      "[4, 10000] loss: 0.185\n",
      "[4, 12000] loss: 0.185\n",
      "Accuracy of the network on train images: 63 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[5,  2000] loss: 0.168\n",
      "[5,  4000] loss: 0.171\n",
      "[5,  6000] loss: 0.173\n",
      "[5,  8000] loss: 0.176\n",
      "[5, 10000] loss: 0.174\n",
      "[5, 12000] loss: 0.178\n",
      "Accuracy of the network on train images: 67 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[6,  2000] loss: 0.155\n",
      "[6,  4000] loss: 0.159\n",
      "[6,  6000] loss: 0.164\n",
      "[6,  8000] loss: 0.159\n",
      "[6, 10000] loss: 0.162\n",
      "[6, 12000] loss: 0.168\n",
      "Accuracy of the network on train images: 70 %\n",
      "Accuracy of the network on test images: 52 %\n",
      "[7,  2000] loss: 0.144\n",
      "[7,  4000] loss: 0.149\n",
      "[7,  6000] loss: 0.150\n",
      "[7,  8000] loss: 0.152\n",
      "[7, 10000] loss: 0.156\n",
      "[7, 12000] loss: 0.155\n",
      "Accuracy of the network on train images: 72 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[8,  2000] loss: 0.135\n",
      "[8,  4000] loss: 0.137\n",
      "[8,  6000] loss: 0.137\n",
      "[8,  8000] loss: 0.142\n",
      "[8, 10000] loss: 0.146\n",
      "[8, 12000] loss: 0.145\n",
      "Accuracy of the network on train images: 74 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[9,  2000] loss: 0.122\n",
      "[9,  4000] loss: 0.126\n",
      "[9,  6000] loss: 0.130\n",
      "[9,  8000] loss: 0.134\n",
      "[9, 10000] loss: 0.133\n",
      "[9, 12000] loss: 0.139\n",
      "Accuracy of the network on train images: 77 %\n",
      "Accuracy of the network on test images: 52 %\n",
      "[10,  2000] loss: 0.111\n",
      "[10,  4000] loss: 0.119\n",
      "[10,  6000] loss: 0.118\n",
      "[10,  8000] loss: 0.123\n",
      "[10, 10000] loss: 0.126\n",
      "[10, 12000] loss: 0.125\n",
      "Accuracy of the network on train images: 79 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[11,  2000] loss: 0.103\n",
      "[11,  4000] loss: 0.106\n",
      "[11,  6000] loss: 0.113\n",
      "[11,  8000] loss: 0.115\n",
      "[11, 10000] loss: 0.114\n",
      "[11, 12000] loss: 0.118\n",
      "Accuracy of the network on train images: 79 %\n",
      "Accuracy of the network on test images: 52 %\n",
      "[12,  2000] loss: 0.095\n",
      "[12,  4000] loss: 0.097\n",
      "[12,  6000] loss: 0.100\n",
      "[12,  8000] loss: 0.105\n",
      "[12, 10000] loss: 0.104\n",
      "[12, 12000] loss: 0.110\n",
      "Accuracy of the network on train images: 82 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[13,  2000] loss: 0.088\n",
      "[13,  4000] loss: 0.091\n",
      "[13,  6000] loss: 0.093\n",
      "[13,  8000] loss: 0.097\n",
      "[13, 10000] loss: 0.098\n",
      "[13, 12000] loss: 0.099\n",
      "Accuracy of the network on train images: 85 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[14,  2000] loss: 0.076\n",
      "[14,  4000] loss: 0.082\n",
      "[14,  6000] loss: 0.087\n",
      "[14,  8000] loss: 0.090\n",
      "[14, 10000] loss: 0.091\n",
      "[14, 12000] loss: 0.100\n",
      "Accuracy of the network on train images: 85 %\n",
      "Accuracy of the network on test images: 51 %\n",
      "[15,  2000] loss: 0.072\n",
      "[15,  4000] loss: 0.072\n",
      "[15,  6000] loss: 0.077\n",
      "[15,  8000] loss: 0.086\n",
      "[15, 10000] loss: 0.085\n",
      "[15, 12000] loss: 0.087\n",
      "Accuracy of the network on train images: 88 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[16,  2000] loss: 0.063\n",
      "[16,  4000] loss: 0.066\n",
      "[16,  6000] loss: 0.075\n",
      "[16,  8000] loss: 0.079\n",
      "[16, 10000] loss: 0.079\n",
      "[16, 12000] loss: 0.079\n",
      "Accuracy of the network on train images: 87 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[17,  2000] loss: 0.062\n",
      "[17,  4000] loss: 0.062\n",
      "[17,  6000] loss: 0.068\n",
      "[17,  8000] loss: 0.068\n",
      "[17, 10000] loss: 0.075\n",
      "[17, 12000] loss: 0.073\n",
      "Accuracy of the network on train images: 90 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[18,  2000] loss: 0.054\n",
      "[18,  4000] loss: 0.059\n",
      "[18,  6000] loss: 0.062\n",
      "[18,  8000] loss: 0.066\n",
      "[18, 10000] loss: 0.068\n",
      "[18, 12000] loss: 0.073\n",
      "Accuracy of the network on train images: 89 %\n",
      "Accuracy of the network on test images: 52 %\n",
      "[19,  2000] loss: 0.054\n",
      "[19,  4000] loss: 0.052\n",
      "[19,  6000] loss: 0.055\n",
      "[19,  8000] loss: 0.054\n",
      "[19, 10000] loss: 0.065\n",
      "[19, 12000] loss: 0.065\n",
      "Accuracy of the network on train images: 90 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[20,  2000] loss: 0.046\n",
      "[20,  4000] loss: 0.048\n",
      "[20,  6000] loss: 0.052\n",
      "[20,  8000] loss: 0.060\n",
      "[20, 10000] loss: 0.058\n",
      "[20, 12000] loss: 0.060\n",
      "Accuracy of the network on train images: 90 %\n",
      "Accuracy of the network on test images: 52 %\n",
      "Finished Training!\n",
      "Accuracy of the network on the 10000 test images: 52 %\n"
     ]
    }
   ],
   "source": [
    "run_experiment([1024, 128], NUM_EPOCHS=20, lr = 0.001, grad_clip=1, drop_wt=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zanvjLDYPj4r",
    "outputId": "c3e06ece-7b0a-4edc-9245-ac0746ce3dc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.275\n",
      "[1,  4000] loss: 0.251\n",
      "[1,  6000] loss: 0.242\n",
      "[1,  8000] loss: 0.242\n",
      "[1, 10000] loss: 0.235\n",
      "[1, 12000] loss: 0.232\n",
      "Accuracy of the network on train images: 50 %\n",
      "Accuracy of the network on test images: 47 %\n",
      "[2,  2000] loss: 0.213\n",
      "[2,  4000] loss: 0.216\n",
      "[2,  6000] loss: 0.215\n",
      "[2,  8000] loss: 0.217\n",
      "[2, 10000] loss: 0.213\n",
      "[2, 12000] loss: 0.213\n",
      "Accuracy of the network on train images: 56 %\n",
      "Accuracy of the network on test images: 50 %\n",
      "[3,  2000] loss: 0.196\n",
      "[3,  4000] loss: 0.195\n",
      "[3,  6000] loss: 0.198\n",
      "[3,  8000] loss: 0.196\n",
      "[3, 10000] loss: 0.199\n",
      "[3, 12000] loss: 0.198\n",
      "Accuracy of the network on train images: 60 %\n",
      "Accuracy of the network on test images: 51 %\n",
      "[4,  2000] loss: 0.179\n",
      "[4,  4000] loss: 0.181\n",
      "[4,  6000] loss: 0.183\n",
      "[4,  8000] loss: 0.181\n",
      "[4, 10000] loss: 0.185\n",
      "[4, 12000] loss: 0.185\n",
      "Accuracy of the network on train images: 62 %\n",
      "Accuracy of the network on test images: 50 %\n",
      "[5,  2000] loss: 0.165\n",
      "[5,  4000] loss: 0.164\n",
      "[5,  6000] loss: 0.169\n",
      "[5,  8000] loss: 0.168\n",
      "[5, 10000] loss: 0.169\n",
      "[5, 12000] loss: 0.173\n",
      "Accuracy of the network on train images: 69 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[6,  2000] loss: 0.148\n",
      "[6,  4000] loss: 0.153\n",
      "[6,  6000] loss: 0.154\n",
      "[6,  8000] loss: 0.155\n",
      "[6, 10000] loss: 0.162\n",
      "[6, 12000] loss: 0.156\n",
      "Accuracy of the network on train images: 71 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[7,  2000] loss: 0.130\n",
      "[7,  4000] loss: 0.139\n",
      "[7,  6000] loss: 0.142\n",
      "[7,  8000] loss: 0.143\n",
      "[7, 10000] loss: 0.146\n",
      "[7, 12000] loss: 0.146\n",
      "Accuracy of the network on train images: 74 %\n",
      "Accuracy of the network on test images: 52 %\n",
      "[8,  2000] loss: 0.124\n",
      "[8,  4000] loss: 0.125\n",
      "[8,  6000] loss: 0.128\n",
      "[8,  8000] loss: 0.131\n",
      "[8, 10000] loss: 0.133\n",
      "[8, 12000] loss: 0.133\n",
      "Accuracy of the network on train images: 78 %\n",
      "Accuracy of the network on test images: 52 %\n",
      "[9,  2000] loss: 0.107\n",
      "[9,  4000] loss: 0.111\n",
      "[9,  6000] loss: 0.117\n",
      "[9,  8000] loss: 0.119\n",
      "[9, 10000] loss: 0.118\n",
      "[9, 12000] loss: 0.125\n",
      "Accuracy of the network on train images: 79 %\n",
      "Accuracy of the network on test images: 52 %\n",
      "[10,  2000] loss: 0.095\n",
      "[10,  4000] loss: 0.100\n",
      "[10,  6000] loss: 0.103\n",
      "[10,  8000] loss: 0.108\n",
      "[10, 10000] loss: 0.109\n",
      "[10, 12000] loss: 0.111\n",
      "Accuracy of the network on train images: 83 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[11,  2000] loss: 0.081\n",
      "[11,  4000] loss: 0.089\n",
      "[11,  6000] loss: 0.093\n",
      "[11,  8000] loss: 0.095\n",
      "[11, 10000] loss: 0.102\n",
      "[11, 12000] loss: 0.102\n",
      "Accuracy of the network on train images: 85 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[12,  2000] loss: 0.075\n",
      "[12,  4000] loss: 0.078\n",
      "[12,  6000] loss: 0.085\n",
      "[12,  8000] loss: 0.088\n",
      "[12, 10000] loss: 0.089\n",
      "[12, 12000] loss: 0.095\n",
      "Accuracy of the network on train images: 86 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[13,  2000] loss: 0.064\n",
      "[13,  4000] loss: 0.069\n",
      "[13,  6000] loss: 0.072\n",
      "[13,  8000] loss: 0.077\n",
      "[13, 10000] loss: 0.082\n",
      "[13, 12000] loss: 0.085\n",
      "Accuracy of the network on train images: 88 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[14,  2000] loss: 0.058\n",
      "[14,  4000] loss: 0.061\n",
      "[14,  6000] loss: 0.070\n",
      "[14,  8000] loss: 0.070\n",
      "[14, 10000] loss: 0.074\n",
      "[14, 12000] loss: 0.076\n",
      "Accuracy of the network on train images: 89 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[15,  2000] loss: 0.051\n",
      "[15,  4000] loss: 0.056\n",
      "[15,  6000] loss: 0.061\n",
      "[15,  8000] loss: 0.060\n",
      "[15, 10000] loss: 0.067\n",
      "[15, 12000] loss: 0.070\n",
      "Accuracy of the network on train images: 90 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[16,  2000] loss: 0.045\n",
      "[16,  4000] loss: 0.047\n",
      "[16,  6000] loss: 0.054\n",
      "[16,  8000] loss: 0.060\n",
      "[16, 10000] loss: 0.058\n",
      "[16, 12000] loss: 0.061\n",
      "Accuracy of the network on train images: 91 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[17,  2000] loss: 0.043\n",
      "[17,  4000] loss: 0.044\n",
      "[17,  6000] loss: 0.046\n",
      "[17,  8000] loss: 0.052\n",
      "[17, 10000] loss: 0.054\n",
      "[17, 12000] loss: 0.057\n",
      "Accuracy of the network on train images: 92 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[18,  2000] loss: 0.038\n",
      "[18,  4000] loss: 0.039\n",
      "[18,  6000] loss: 0.041\n",
      "[18,  8000] loss: 0.047\n",
      "[18, 10000] loss: 0.057\n",
      "[18, 12000] loss: 0.054\n",
      "Accuracy of the network on train images: 92 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[19,  2000] loss: 0.035\n",
      "[19,  4000] loss: 0.035\n",
      "[19,  6000] loss: 0.041\n",
      "[19,  8000] loss: 0.046\n",
      "[19, 10000] loss: 0.048\n",
      "[19, 12000] loss: 0.049\n",
      "Accuracy of the network on train images: 94 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "[20,  2000] loss: 0.030\n",
      "[20,  4000] loss: 0.035\n",
      "[20,  6000] loss: 0.037\n",
      "[20,  8000] loss: 0.037\n",
      "[20, 10000] loss: 0.045\n",
      "[20, 12000] loss: 0.051\n",
      "Accuracy of the network on train images: 93 %\n",
      "Accuracy of the network on test images: 53 %\n",
      "Finished Training!\n",
      "Accuracy of the network on the 10000 test images: 53 %\n"
     ]
    }
   ],
   "source": [
    "run_experiment([1024, 512, 128], NUM_EPOCHS=20, lr = 0.001, grad_clip=1, drop_wt=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "colab_type": "code",
    "id": "Kt50f1t_O9Hj",
    "outputId": "4e968a01-780f-493b-e1bc-d5a609ca1fb0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-a32b89b4fcde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'acc_dict' is not defined"
     ]
    }
   ],
   "source": [
    "print(acc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0lODpTeLX0An"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Emi_COMP 551 - MLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (sigtyp)",
   "language": "python",
   "name": "sigtyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
